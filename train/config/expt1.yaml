model_name: "google/gemma-3-12b-it"
dataset_name: "anag007/asmitai_konkani_gemma-3-12b_noisified_alpaca_instruction_data"
experiment_parent_path: "experiments"
experiment_name: "asmitai_konkani_gemma-3-12b-it.expt1"

train:
  max_seq_length: 2048
  load_in_4bit: true
  load_in_8bit: false
  full_finetuning: false

lora:
  r: 8
  lora_alpha: 8
  lora_dropout: 0
  bias: "none"
  random_state: 3407

sft_config:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  warmup_steps: 1000
  save_steps: 500
  eval_steps: 500
  logging_steps: 500
  num_train_epochs: 5
  learning_rate: 0.0002
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  seed: 3407
  report_to: ["tensorboard", "wandb"]
  dataset_num_proc: 2

wandb:
  project: "asmitai-konkani-llm"
  name: "asmitai-gemma-3-12b_noisified_alpaca_instruction_data.expt1"
